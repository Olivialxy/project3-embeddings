{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model trained on wikipedia data\n",
    "\n",
    "model = gensim.downloader.load(\"glove-wiki-gigaword-100\")\n",
    "# Getting the vector for a word\n",
    "\n",
    "dog = model['dog']\n",
    "\n",
    "# Getting the most similar words\n",
    "\n",
    "print(model.most_similar('dog'))\n",
    "\n",
    "# Getting the similarity between two words\n",
    "\n",
    "print(model.similarity('dog', 'cat'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#find the most similar words to a word of your choice. \n",
    "#Does the result make sense? Look up some of the words if you don't know them\n",
    "#Share and discuss with your group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papaya = model['papaya']\n",
    "print(model.most_similar('papaya'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juice = model['juice']\n",
    "print(model.most_similar('juice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpop = model['kpop']\n",
    "print(model.most_similar('kpop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thailand = model['thailand']\n",
    "print(model.most_similar('thailand'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we can extract a \"capital vector\" by subtracting the vector for a country from the vector for its capital\n",
    "capital= model[\"berlin\"]-model['germany']\n",
    "\n",
    "fra_capital=model[\"france\"]+capital\n",
    "\n",
    "print(model.most_similar(fra_capital))\n",
    "#likewise, we can extract a \"plural vector\" by subtracting the vector for a singular noun from the vector for its plural\n",
    "plural=model[\"cats\"]-model[\"cat\"]\n",
    "\n",
    "print(model.most_similar(plural))\n",
    "\n",
    "#we can also add vectors together\n",
    "plural_dogs=model[\"dog\"]+plural\n",
    "\n",
    "print(model.most_similar(plural_dogs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be used to find examples of data bias in word embeddings\n",
    "\n",
    "# for example, we can find the most similar words to \"doctor\" and \"nurse\"\n",
    "\n",
    "gender=model[\"woman\"]-model[\"man\"]\n",
    "\n",
    "print(model.most_similar(gender))\n",
    "print(model.most_similar(-gender))\n",
    "\n",
    "model.most_similar(model[\"doctor\"]+gender)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to isolate a specific dimension in the embeddings and add it to multiple words. \n",
    "#does this show any bias or assumptions in the data? \n",
    "# Discuss with your group and share most interesting dimension in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plural=model[\"cats\"]-model[\"cat\"]\n",
    "plural_papayas = model[\"papaya\"]+plural\n",
    "print(model.most_similar(plural_papayas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plural_juices = model[\"juice\"]+plural\n",
    "print(model.most_similar(plural_juices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plural_kpops = model[\"kpop\"]+plural\n",
    "print(model.most_similar(plural_kpops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plural_thailands = model[\"thailand\"]+plural\n",
    "print(model.most_similar(plural_thailands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Visualization and Embeddings in Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize pca of word embeddings\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words=[\"dog\",\"cat\",\"fish\", \"whale\", \"bird\", \"eagle\", \"lion\", \"tiger\", \"cheetah\", \"elephant\", \"giraffe\", \"zebra\", \"kangaroo\", \"koala\", \"panda\", \"monkey\", \"gorilla\", \"chimpanzee\", \"orangutan\", ]\n",
    "\n",
    "X=[model[word] for word in words]\n",
    "\n",
    "#identify clusters of words in the word embeddings at high dimensions\n",
    "\n",
    "pca=PCA(n_components=2)\n",
    "\n",
    "X_pca=pca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (X_pca[i,0],X_pca[i,1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat this process with your own set of 10-20 words. \n",
    "#Does the proximity between words and overall distances make sense? Work as a group and discuss your individual findings.\n",
    "words=[\"banana\", \"mountain\", \"robot\", \"beach\", \"galaxy\", \"engineer\", \"happiness\", \"piano\", \"airplane\", \"library\", \"desert\", \"philosophy\", \"whale\", \"museum\", \"molecule\"]\n",
    "\n",
    "X=[model[word] for word in words]\n",
    "\n",
    "#identify clusters of words in the word embeddings at high dimensions\n",
    "\n",
    "pca=PCA(n_components=2)\n",
    "\n",
    "X_pca=pca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (X_pca[i,0],X_pca[i,1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the process with a different pre-trained model and the same set of words. \n",
    "# What has changed in the embedding space? What might this have to do with the training data?\n",
    "twitter_model= gensim.downloader.load(\"glove-twitter-100\")\n",
    "words=[\"banana\", \"mountain\", \"robot\", \"beach\", \"galaxy\", \"engineer\", \"happiness\", \"piano\", \"airplane\", \"library\", \"desert\", \"philosophy\", \"whale\", \"museum\", \"molecule\"]\n",
    "\n",
    "X=[twitter_model[word] for word in words]\n",
    "\n",
    "#identify clusters of words in the word embeddings at high dimensions\n",
    "\n",
    "pca=PCA(n_components=2)\n",
    "\n",
    "X_pca=pca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (X_pca[i,0],X_pca[i,1]))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Embedding reddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in uiuc csv\n",
    "import pandas as pd\n",
    "\n",
    "uiuc=pd.read_csv(\"uiuc.csv\")\n",
    "mich=pd.read_csv(\"umich.csv\")\n",
    "\n",
    "#sample so we have even number of samples from each dataset\n",
    "mich=mich.sample(n=4725)\n",
    "\n",
    "#assign labels based on origin subreddit of comment\n",
    "uiuc['label']=1\n",
    "mich['label']=0\n",
    "\n",
    "#you will be working with the data csv for the rest of the question\n",
    "data=pd.concat([uiuc,mich])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the code from project 1, clean the text and create a bag of words representation of the text\n",
    "def clean(text):\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "data['cleaned_text'] = data['text'].apply(clean)\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(data['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the embedding code from last week, plot a pca of all posts in the subreddits data set\n",
    "\n",
    "#color by the \"label\" column to see if the two subreddits are separable in the embedding space\n",
    "\n",
    "# is there a clear separation between the two columns? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 2000\n",
    "bag_of_words_sampled = bag_of_words[:sample_size].toarray()\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(bag_of_words_sampled)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=data['label'][:sample_size], cmap=\"coolwarm\", alpha=0.5)\n",
    "plt.colorbar(label=\"Label\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"PCA of Sampled UIUC and UMich Subreddits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is a function to calculate the average embedding of a cleaned document\n",
    "import numpy as np\n",
    "#we can sum together all the embeddings to get a representation of all the concepts expressed in the document\n",
    "def avg_embedding(sentence, model):\n",
    "    words=sentence.split()\n",
    "    embeddings=[model[word] for word in words if word in model]\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "sentences=[\"i am running today\", \"i will jog tomorrow\", \"i ran yesterday\", \"i love drawing\", \"i adore painting\", \"i used to like pottery\"]\n",
    "\n",
    "#we can then calculate the average embedding of each sentence\n",
    "\n",
    "avg_embeddings=[avg_embedding(sentence, model) for sentence in sentences]\n",
    "\n",
    "#we can then plot the pca of these embeddings\n",
    "\n",
    "pca=PCA(n_components=2)\n",
    "\n",
    "X_pca=pca.fit_transform(avg_embeddings)\n",
    "\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1])\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    plt.annotate(sentence, (X_pca[i,0],X_pca[i,1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#use this function to calculate the average embedding of each post in the dataset\n",
    "\n",
    "#plot the pca of the average embeddings, color by the \"label\" column\n",
    "\n",
    "#how is this plot different from the previous one?\n",
    "\n",
    "#is there clear separation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is a function to calculate the average embedding of a cleaned document\n",
    "\n",
    "data['avg_embedding'] = data['cleaned_text'].apply(lambda x: avg_embedding(x, model))\n",
    "avg_embeddings=[avg_embedding(sentence, model) for sentence in data['cleaned_text']]\n",
    "avg_embeddings=[embedding for embedding in avg_embeddings if not np.isnan(embedding).any()]\n",
    "\n",
    "pca=PCA(n_components=2)\n",
    "\n",
    "X_pca=pca.fit_transform(avg_embeddings)\n",
    "\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1])\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    plt.annotate(sentence, (X_pca[i,0],X_pca[i,1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: HuggingFace and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please wait until after next week to start\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "#download bert model\n",
    "pipe = pipeline(\"text-classification\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the pipeline to classify the sentiment of a sentence of your choice\n",
    "pipe(\"I love this class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the pipeline to classify the sentiment of all posts in the reddit dataset\n",
    "#report the most common sentiment label\n",
    "def classify_sentiment_batch(texts, batch_size=32):\n",
    "    sentiments = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_texts = [text[:512] if text.strip() else \"neutral\" for text in batch_texts]\n",
    "        try:\n",
    "            batch_results = pipe(batch_texts)\n",
    "            batch_labels = [result['label'] for result in batch_results]\n",
    "            sentiments.extend(batch_labels)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i // batch_size}: {e}\")\n",
    "            sentiments.extend([\"ERROR\"] * len(batch_texts))\n",
    "    return sentiments\n",
    "\n",
    "data['sentiment'] = classify_sentiment_batch(data['cleaned_text'].tolist())\n",
    "most_common_sentiment = data['sentiment'].mode()[0]\n",
    "print(f\"The most common sentiment label is: {most_common_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at ten sample posts and labels; do you agree with all of them? \n",
    "sample_posts = data[['text', 'sentiment']].sample(n=10)\n",
    "for index, row in sample_posts.iterrows():\n",
    "    print(f\"Post: {row['text']}\")\n",
    "    print(f\"Predicted Sentiment: {row['sentiment']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is the UIUC or Michigan subreddit more negative?\n",
    "uiuc_negative = data[(data['label'] == 1) & (data['sentiment'] == 'NEG')].shape[0]\n",
    "mich_negative = data[(data['label'] == 0) & (data['sentiment'] == 'NEG')].shape[0]\n",
    "\n",
    "uiuc_total = data[data['label'] == 1].shape[0]\n",
    "mich_total = data[data['label'] == 0].shape[0]\n",
    "\n",
    "uiuc_negativity = uiuc_negative / uiuc_total\n",
    "mich_negativity = mich_negative / mich_total \n",
    "\n",
    "uiuc_negativity\n",
    "mich_negativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
